{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/petpoint/blob/main/PetpointWithGeneralConver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bryanbayup/petpoint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRhMCXKVL_0Q",
        "outputId": "a4aa710f-061a-4264-9889-3adc6f92ae62"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'petpoint' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import glob"
      ],
      "metadata": {
        "id": "kY11deKtXCYF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List untuk menyimpan pasangan percakapan\n",
        "conversation_pairs = []"
      ],
      "metadata": {
        "id": "R2TAPmbwXC1b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk mengekstrak pasangan percakapan dari intents\n",
        "def extract_conversation_pairs(intents):\n",
        "    pairs = []\n",
        "    for intent in intents:\n",
        "        for utterance in intent['utterances']:\n",
        "            for answer in intent['answers']:\n",
        "                pairs.append((utterance, answer))\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "OmKJypP3XFyv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load semua JSON files dari corpus/id/\n",
        "intents = []\n",
        "for file in glob.glob('/content/petpoint/corpus/id/*.json'):\n",
        "    with open(file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "        intents.extend(data)"
      ],
      "metadata": {
        "id": "_-2MMx6QXLKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load domain-specific data\n",
        "with open('/content/petpoint/kucing_anjing/kucing_anjing.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "    intents.extend(data)"
      ],
      "metadata": {
        "id": "YdWW_dm0XOXn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekstrak pasangan percakapan\n",
        "conversation_pairs.extend(extract_conversation_pairs(intents))"
      ],
      "metadata": {
        "id": "vgCBMeBiXSXZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat DataFrame\n",
        "df = pd.DataFrame(conversation_pairs, columns=['input', 'response'])"
      ],
      "metadata": {
        "id": "FT4015RwXsbD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load normalization dictionary\n",
        "normalization_dict = {}\n",
        "with open('/content/petpoint/normalization/normalization.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        # Check if the line contains the delimiter before splitting\n",
        "        if '\\t' in line:\n",
        "            slang, normal = line.strip().split('\\t')\n",
        "            normalization_dict[slang] = normal\n",
        "        else:\n",
        "            print(f\"Warning: Skipping line '{line.strip()}', no tab delimiter found.\")"
      ],
      "metadata": {
        "id": "9258GAKDYBH5",
        "outputId": "4f32e1a8-3bf4-4cfd-e865-46e4daa64d1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Skipping line 'kuy ayo', no tab delimiter found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to normalize text\n",
        "def normalize_text(text):\n",
        "    words = text.split()\n",
        "    normalized_words = [normalization_dict.get(word, word) for word in words]\n",
        "    return ' '.join(normalized_words)"
      ],
      "metadata": {
        "id": "BYT56VnYYEog"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load stopwords\n",
        "with open('/content/petpoint/normalization/stopword.txt', 'r', encoding='utf-8') as f:\n",
        "    stopwords = set(f.read().splitlines())"
      ],
      "metadata": {
        "id": "w2p6HDFGYHPs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stopwords]\n",
        "    return ' '.join(filtered_words)"
      ],
      "metadata": {
        "id": "h6plXALSZFiZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "\n",
        "# Initialize the Indonesian pipeline\n",
        "nlp = stanza.Pipeline('id')\n",
        "\n",
        "def tokenize_and_lemmatize(text):\n",
        "    doc = nlp(text)\n",
        "    lemmas = [word.lemma for sentence in doc.sentences for word in sentence.words]\n",
        "    return ' '.join(lemmas)"
      ],
      "metadata": {
        "id": "e1duBNv4ZISk",
        "outputId": "ed686f95-4638-40e9-8fe8-1d917eaad35e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729,
          "referenced_widgets": [
            "723d40fe9768418993d4c3c8262fb330",
            "967f4028e6cd44c1be99ed680aefa3ad",
            "89dfc4c905bf40608e187bc84676c031",
            "89afb1e7c8b444668076e2aa5f29d21f",
            "7123e06c13de451b8d4e1807ca1c10f4",
            "c242a3506d7c48ecb62e76273ad61ca0",
            "1d576c0dbef34a9cae2bb857bdd1e680",
            "9c067e7dfebf45ff88189e245c12f282",
            "66b3102f70f64ced99b1fed435f373c7",
            "fbee367deb8c4b94b0addb795145be54",
            "825f1a54e91e4f9f8fa8435e043b6a83"
          ]
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   â€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "723d40fe9768418993d4c3c8262fb330"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: id (Indonesian):\n",
            "===============================\n",
            "| Processor    | Package      |\n",
            "-------------------------------\n",
            "| tokenize     | gsd          |\n",
            "| mwt          | gsd          |\n",
            "| pos          | gsd_charlm   |\n",
            "| lemma        | gsd_nocharlm |\n",
            "| constituency | icon_charlm  |\n",
            "| depparse     | gsd_charlm   |\n",
            "===============================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "/usr/local/lib/python3.10/dist-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "INFO:stanza:Loading: mwt\n",
            "/usr/local/lib/python3.10/dist-packages/stanza/models/mwt/trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "INFO:stanza:Loading: pos\n",
            "/usr/local/lib/python3.10/dist-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "/usr/local/lib/python3.10/dist-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
            "/usr/local/lib/python3.10/dist-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(filename, lambda storage, loc: storage)\n",
            "INFO:stanza:Loading: lemma\n",
            "/usr/local/lib/python3.10/dist-packages/stanza/models/lemma/trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "INFO:stanza:Loading: constituency\n",
            "/usr/local/lib/python3.10/dist-packages/stanza/models/constituency/base_trainer.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "INFO:stanza:Loading: depparse\n",
            "/usr/local/lib/python3.10/dist-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n",
        "import stanza\n",
        "stanza.download('id')"
      ],
      "metadata": {
        "id": "YtAGeppQZKss",
        "outputId": "5f80a7df-c1da-4e7d-9d63-36e38770009f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466,
          "referenced_widgets": [
            "c01529b049b74c78897d68958458a78e",
            "7d7cc9acadee42aeb31b437820836b0c",
            "401dbf08560b471ea81a9525038c002b",
            "508754262cac4e9c8d5cf8f442cee47d",
            "1dab72a84e9c41ec9ee9d3c7638d74d3",
            "b6ce266cc18b4b2c9d0ce19255091412",
            "c343dfdfb516426495c195655a9634f6",
            "d9de287aef6c4a07bbabf652dbc98a7a",
            "abaf393c6395493f987dfd47c63c207b",
            "9cc1933671c141fd8504bf4eee44bf3e",
            "d2f474822556412fafe0c4c4ace29ca6"
          ]
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.10/dist-packages (1.9.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from stanza) (2.14.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.6)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from stanza) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   â€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c01529b049b74c78897d68958458a78e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: id (Indonesian) ...\n",
            "INFO:stanza:File exists: /root/stanza_resources/id/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Normalisasi\n",
        "    text = normalize_text(text.lower())\n",
        "    # Hapus stopwords\n",
        "    text = remove_stopwords(text)\n",
        "    # Tokenisasi dan Lematisasi\n",
        "    text = tokenize_and_lemmatize(text)\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df['input'] = df['input'].apply(preprocess_text)\n",
        "df['response'] = df['response'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "aEp2bk34ZTc6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "Ig7iOu7vZXgQ",
        "outputId": "556ae5e4-7494-4988-9b0d-d402026a741c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "6lOWCEoqZimI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "def build_vocab(sentences, max_vocab_size=5000):\n",
        "    word_counts = Counter(itertools.chain(*[s.split() for s in sentences]))\n",
        "    most_common = word_counts.most_common(max_vocab_size)\n",
        "    idx2word = ['<PAD>', '<SOS>', '<EOS>', '<UNK>'] + [word for word, _ in most_common]\n",
        "    word2idx = {word: idx for idx, word in enumerate(idx2word)}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "# Bangun vocabulary untuk input dan output\n",
        "input_word2idx, input_idx2word = build_vocab(df['input'])\n",
        "output_word2idx, output_idx2word = build_vocab(df['response'])"
      ],
      "metadata": {
        "id": "IIdfb1eSZmFp"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_indices(sentence, word2idx):\n",
        "    indices = [word2idx.get(word, word2idx['<UNK>']) for word in sentence.split()]\n",
        "    return indices\n",
        "\n",
        "df['input_indices'] = df['input'].apply(lambda x: sentence_to_indices(x, input_word2idx))\n",
        "df['response_indices'] = df['response'].apply(lambda x: [output_word2idx['<SOS>']] + sentence_to_indices(x, output_word2idx) + [output_word2idx['<EOS>']])"
      ],
      "metadata": {
        "id": "TskYUbJIZpw8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, inputs, outputs):\n",
        "        self.inputs = inputs\n",
        "        self.outputs = outputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.inputs[idx], dtype=torch.long), torch.tensor(self.outputs[idx], dtype=torch.long)\n",
        "\n",
        "dataset = ChatDataset(df['input_indices'].tolist(), df['response_indices'].tolist())\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda x: x)"
      ],
      "metadata": {
        "id": "DrbgeL7JZwnG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, hidden_size, num_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_length]\n",
        "        embedding = self.embedding(x)\n",
        "        outputs, (hidden, cell) = self.lstm(embedding)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embed_size, hidden_size, num_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        # x: [batch_size]\n",
        "        x = x.unsqueeze(1)  # [batch_size, 1]\n",
        "        embedding = self.embedding(x)\n",
        "        outputs, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
        "        predictions = self.fc(outputs.squeeze(1))\n",
        "        return predictions, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, output_word2idx):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.output_word2idx = output_word2idx\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "        batch_size = source.size(0)\n",
        "        target_len = target.size(1)\n",
        "        target_vocab_size = len(self.output_word2idx)\n",
        "\n",
        "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(device)\n",
        "        hidden, cell = self.encoder(source)\n",
        "\n",
        "        x = target[:, 0]\n",
        "\n",
        "        for t in range(1, target_len):\n",
        "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
        "            outputs[:, t] = output\n",
        "            best_guess = output.argmax(1)\n",
        "            x = target[:, t] if torch.rand(1).item() < teacher_forcing_ratio else best_guess\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "4Wvt4M6tdz0O"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "import torch.optim as optim\n",
        "input_size_encoder = len(input_word2idx)\n",
        "input_size_decoder = len(output_word2idx)\n",
        "output_size = len(output_word2idx)\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "encoder_net = Encoder(input_size_encoder, embed_size, hidden_size, num_layers).to(device)\n",
        "decoder_net = Decoder(output_size, embed_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "model = Seq2Seq(encoder_net, decoder_net, output_word2idx).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=output_word2idx['<PAD>'])"
      ],
      "metadata": {
        "id": "b9tPWjwhd6re"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        # Ambil batch data\n",
        "        inputs, targets = zip(*batch)\n",
        "        inputs = nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=input_word2idx['<PAD>']).to(device)\n",
        "        targets = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=output_word2idx['<PAD>']).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs, targets)\n",
        "\n",
        "        # Reshape untuk menghitung loss\n",
        "        outputs = outputs[:, 1:].reshape(-1, outputs.shape[2])\n",
        "        targets = targets[:, 1:].reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "s79D4Frvd9iO",
        "outputId": "e448ce58-3269-4431-e7a9-faa5c227877c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100]\n",
            "Batch 0/25, Loss: 6.0538\n",
            "Epoch [2/100]\n",
            "Batch 0/25, Loss: 4.3893\n",
            "Epoch [3/100]\n",
            "Batch 0/25, Loss: 3.7374\n",
            "Epoch [4/100]\n",
            "Batch 0/25, Loss: 3.9797\n",
            "Epoch [5/100]\n",
            "Batch 0/25, Loss: 3.2109\n",
            "Epoch [6/100]\n",
            "Batch 0/25, Loss: 2.9435\n",
            "Epoch [7/100]\n",
            "Batch 0/25, Loss: 2.7337\n",
            "Epoch [8/100]\n",
            "Batch 0/25, Loss: 2.5754\n",
            "Epoch [9/100]\n",
            "Batch 0/25, Loss: 2.1392\n",
            "Epoch [10/100]\n",
            "Batch 0/25, Loss: 1.6517\n",
            "Epoch [11/100]\n",
            "Batch 0/25, Loss: 2.1491\n",
            "Epoch [12/100]\n",
            "Batch 0/25, Loss: 0.8187\n",
            "Epoch [13/100]\n",
            "Batch 0/25, Loss: 1.0773\n",
            "Epoch [14/100]\n",
            "Batch 0/25, Loss: 1.2805\n",
            "Epoch [15/100]\n",
            "Batch 0/25, Loss: 1.1590\n",
            "Epoch [16/100]\n",
            "Batch 0/25, Loss: 0.5523\n",
            "Epoch [17/100]\n",
            "Batch 0/25, Loss: 0.4831\n",
            "Epoch [18/100]\n",
            "Batch 0/25, Loss: 0.7559\n",
            "Epoch [19/100]\n",
            "Batch 0/25, Loss: 0.4969\n",
            "Epoch [20/100]\n",
            "Batch 0/25, Loss: 0.3064\n",
            "Epoch [21/100]\n",
            "Batch 0/25, Loss: 0.3873\n",
            "Epoch [22/100]\n",
            "Batch 0/25, Loss: 0.6354\n",
            "Epoch [23/100]\n",
            "Batch 0/25, Loss: 0.4686\n",
            "Epoch [24/100]\n",
            "Batch 0/25, Loss: 0.3012\n",
            "Epoch [25/100]\n",
            "Batch 0/25, Loss: 0.2571\n",
            "Epoch [26/100]\n",
            "Batch 0/25, Loss: 0.3043\n",
            "Epoch [27/100]\n",
            "Batch 0/25, Loss: 0.4078\n",
            "Epoch [28/100]\n",
            "Batch 0/25, Loss: 0.7485\n",
            "Epoch [29/100]\n",
            "Batch 0/25, Loss: 0.7119\n",
            "Epoch [30/100]\n",
            "Batch 0/25, Loss: 0.3301\n",
            "Epoch [31/100]\n",
            "Batch 0/25, Loss: 0.3274\n",
            "Epoch [32/100]\n",
            "Batch 0/25, Loss: 0.2200\n",
            "Epoch [33/100]\n",
            "Batch 0/25, Loss: 0.2208\n",
            "Epoch [34/100]\n",
            "Batch 0/25, Loss: 0.3420\n",
            "Epoch [35/100]\n",
            "Batch 0/25, Loss: 0.3436\n",
            "Epoch [36/100]\n",
            "Batch 0/25, Loss: 0.2449\n",
            "Epoch [37/100]\n",
            "Batch 0/25, Loss: 0.2519\n",
            "Epoch [38/100]\n",
            "Batch 0/25, Loss: 0.3923\n",
            "Epoch [39/100]\n",
            "Batch 0/25, Loss: 0.1967\n",
            "Epoch [40/100]\n",
            "Batch 0/25, Loss: 0.1338\n",
            "Epoch [41/100]\n",
            "Batch 0/25, Loss: 0.3136\n",
            "Epoch [42/100]\n",
            "Batch 0/25, Loss: 0.3311\n",
            "Epoch [43/100]\n",
            "Batch 0/25, Loss: 0.2550\n",
            "Epoch [44/100]\n",
            "Batch 0/25, Loss: 0.2512\n",
            "Epoch [45/100]\n",
            "Batch 0/25, Loss: 0.4703\n",
            "Epoch [46/100]\n",
            "Batch 0/25, Loss: 0.1912\n",
            "Epoch [47/100]\n",
            "Batch 0/25, Loss: 0.3024\n",
            "Epoch [48/100]\n",
            "Batch 0/25, Loss: 0.2049\n",
            "Epoch [49/100]\n",
            "Batch 0/25, Loss: 0.3117\n",
            "Epoch [50/100]\n",
            "Batch 0/25, Loss: 0.4927\n",
            "Epoch [51/100]\n",
            "Batch 0/25, Loss: 0.8085\n",
            "Epoch [52/100]\n",
            "Batch 0/25, Loss: 0.1515\n",
            "Epoch [53/100]\n",
            "Batch 0/25, Loss: 0.4703\n",
            "Epoch [54/100]\n",
            "Batch 0/25, Loss: 0.2431\n",
            "Epoch [55/100]\n",
            "Batch 0/25, Loss: 0.2427\n",
            "Epoch [56/100]\n",
            "Batch 0/25, Loss: 0.2474\n",
            "Epoch [57/100]\n",
            "Batch 0/25, Loss: 0.2462\n",
            "Epoch [58/100]\n",
            "Batch 0/25, Loss: 0.5714\n",
            "Epoch [59/100]\n",
            "Batch 0/25, Loss: 0.1921\n",
            "Epoch [60/100]\n",
            "Batch 0/25, Loss: 0.3771\n",
            "Epoch [61/100]\n",
            "Batch 0/25, Loss: 0.2939\n",
            "Epoch [62/100]\n",
            "Batch 0/25, Loss: 0.4700\n",
            "Epoch [63/100]\n",
            "Batch 0/25, Loss: 0.3711\n",
            "Epoch [64/100]\n",
            "Batch 0/25, Loss: 0.2443\n",
            "Epoch [65/100]\n",
            "Batch 0/25, Loss: 0.3581\n",
            "Epoch [66/100]\n",
            "Batch 0/25, Loss: 0.4633\n",
            "Epoch [67/100]\n",
            "Batch 0/25, Loss: 0.3707\n",
            "Epoch [68/100]\n",
            "Batch 0/25, Loss: 0.3716\n",
            "Epoch [69/100]\n",
            "Batch 0/25, Loss: 0.2004\n",
            "Epoch [70/100]\n",
            "Batch 0/25, Loss: 0.3449\n",
            "Epoch [71/100]\n",
            "Batch 0/25, Loss: 0.4048\n",
            "Epoch [72/100]\n",
            "Batch 0/25, Loss: 0.2135\n",
            "Epoch [73/100]\n",
            "Batch 0/25, Loss: 0.3297\n",
            "Epoch [74/100]\n",
            "Batch 0/25, Loss: 0.2973\n",
            "Epoch [75/100]\n",
            "Batch 0/25, Loss: 0.1345\n",
            "Epoch [76/100]\n",
            "Batch 0/25, Loss: 0.3835\n",
            "Epoch [77/100]\n",
            "Batch 0/25, Loss: 0.3637\n",
            "Epoch [78/100]\n",
            "Batch 0/25, Loss: 0.3781\n",
            "Epoch [79/100]\n",
            "Batch 0/25, Loss: 0.2332\n",
            "Epoch [80/100]\n",
            "Batch 0/25, Loss: 0.2149\n",
            "Epoch [81/100]\n",
            "Batch 0/25, Loss: 0.3164\n",
            "Epoch [82/100]\n",
            "Batch 0/25, Loss: 0.4172\n",
            "Epoch [83/100]\n",
            "Batch 0/25, Loss: 0.1759\n",
            "Epoch [84/100]\n",
            "Batch 0/25, Loss: 0.2425\n",
            "Epoch [85/100]\n",
            "Batch 0/25, Loss: 0.4177\n",
            "Epoch [86/100]\n",
            "Batch 0/25, Loss: 0.2365\n",
            "Epoch [87/100]\n",
            "Batch 0/25, Loss: 0.2738\n",
            "Epoch [88/100]\n",
            "Batch 0/25, Loss: 0.2316\n",
            "Epoch [89/100]\n",
            "Batch 0/25, Loss: 0.5215\n",
            "Epoch [90/100]\n",
            "Batch 0/25, Loss: 0.4469\n",
            "Epoch [91/100]\n",
            "Batch 0/25, Loss: 0.3652\n",
            "Epoch [92/100]\n",
            "Batch 0/25, Loss: 0.6655\n",
            "Epoch [93/100]\n",
            "Batch 0/25, Loss: 0.1880\n",
            "Epoch [94/100]\n",
            "Batch 0/25, Loss: 0.3669\n",
            "Epoch [95/100]\n",
            "Batch 0/25, Loss: 0.2899\n",
            "Epoch [96/100]\n",
            "Batch 0/25, Loss: 0.6483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, sentence, input_word2idx, output_idx2word, max_length=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Preprocessing\n",
        "        sentence = preprocess_text(sentence)\n",
        "        # Konversi ke indeks\n",
        "        inputs = torch.tensor([sentence_to_indices(sentence, input_word2idx)], dtype=torch.long).to(device)\n",
        "\n",
        "        hidden, cell = model.encoder(inputs)\n",
        "        x = torch.tensor([output_word2idx['<SOS>']], dtype=torch.long).to(device)\n",
        "        outputs = []\n",
        "        for _ in range(max_length):\n",
        "            output, hidden, cell = model.decoder(x, hidden, cell)\n",
        "            best_guess = output.argmax(1)\n",
        "            if best_guess.item() == output_word2idx['<EOS>']:\n",
        "                break\n",
        "            outputs.append(best_guess.item())\n",
        "            x = best_guess\n",
        "    translated_sentence = ' '.join([output_idx2word[idx] for idx in outputs])\n",
        "    return translated_sentence"
      ],
      "metadata": {
        "id": "ZN6jygzoeVo5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_response_by_sentiment(response, sentiment_score):\n",
        "    if sentiment_score < 0:\n",
        "        response = \"Maaf mendengarnya. \" + response\n",
        "    elif sentiment_score > 0:\n",
        "        response = \"Senang mendengarnya! \" + response\n",
        "    return response"
      ],
      "metadata": {
        "id": "_d_fa1V_fTTt"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.type) for ent in doc.ents]\n",
        "    return entities"
      ],
      "metadata": {
        "id": "txJ_lH1zfWzH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot_response(user_input):\n",
        "    # Analisis sentimen\n",
        "    sentiment_score = analyze_sentiment(user_input)\n",
        "\n",
        "    # Ekstrak entitas\n",
        "    entities = extract_entities(user_input)\n",
        "\n",
        "    # Dapatkan respon dari model\n",
        "    response = translate_sentence(model, user_input, input_word2idx, output_idx2word)\n",
        "\n",
        "    # Sesuaikan respon berdasarkan sentimen\n",
        "    response = adjust_response_by_sentiment(response, sentiment_score)\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "y_HEGbZgfZRl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vh7UdCX8fcVU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNh3DDPZ6/t16U8d2Q1JTCy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "723d40fe9768418993d4c3c8262fb330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_967f4028e6cd44c1be99ed680aefa3ad",
              "IPY_MODEL_89dfc4c905bf40608e187bc84676c031",
              "IPY_MODEL_89afb1e7c8b444668076e2aa5f29d21f"
            ],
            "layout": "IPY_MODEL_7123e06c13de451b8d4e1807ca1c10f4"
          }
        },
        "967f4028e6cd44c1be99ed680aefa3ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c242a3506d7c48ecb62e76273ad61ca0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1d576c0dbef34a9cae2bb857bdd1e680",
            "value": "Downloadingâ€‡https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:â€‡"
          }
        },
        "89dfc4c905bf40608e187bc84676c031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c067e7dfebf45ff88189e245c12f282",
            "max": 48453,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66b3102f70f64ced99b1fed435f373c7",
            "value": 48453
          }
        },
        "89afb1e7c8b444668076e2aa5f29d21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbee367deb8c4b94b0addb795145be54",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_825f1a54e91e4f9f8fa8435e043b6a83",
            "value": "â€‡392k/?â€‡[00:00&lt;00:00,â€‡10.1MB/s]"
          }
        },
        "7123e06c13de451b8d4e1807ca1c10f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c242a3506d7c48ecb62e76273ad61ca0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d576c0dbef34a9cae2bb857bdd1e680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c067e7dfebf45ff88189e245c12f282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b3102f70f64ced99b1fed435f373c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbee367deb8c4b94b0addb795145be54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "825f1a54e91e4f9f8fa8435e043b6a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c01529b049b74c78897d68958458a78e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d7cc9acadee42aeb31b437820836b0c",
              "IPY_MODEL_401dbf08560b471ea81a9525038c002b",
              "IPY_MODEL_508754262cac4e9c8d5cf8f442cee47d"
            ],
            "layout": "IPY_MODEL_1dab72a84e9c41ec9ee9d3c7638d74d3"
          }
        },
        "7d7cc9acadee42aeb31b437820836b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6ce266cc18b4b2c9d0ce19255091412",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c343dfdfb516426495c195655a9634f6",
            "value": "Downloadingâ€‡https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:â€‡"
          }
        },
        "401dbf08560b471ea81a9525038c002b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9de287aef6c4a07bbabf652dbc98a7a",
            "max": 48453,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abaf393c6395493f987dfd47c63c207b",
            "value": 48453
          }
        },
        "508754262cac4e9c8d5cf8f442cee47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cc1933671c141fd8504bf4eee44bf3e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d2f474822556412fafe0c4c4ace29ca6",
            "value": "â€‡392k/?â€‡[00:00&lt;00:00,â€‡12.9MB/s]"
          }
        },
        "1dab72a84e9c41ec9ee9d3c7638d74d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6ce266cc18b4b2c9d0ce19255091412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c343dfdfb516426495c195655a9634f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9de287aef6c4a07bbabf652dbc98a7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abaf393c6395493f987dfd47c63c207b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9cc1933671c141fd8504bf4eee44bf3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2f474822556412fafe0c4c4ace29ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}